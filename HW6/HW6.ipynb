{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RJSSUE/Jobs-to-be-done-1/blob/master/HW6/HW6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Homework 6: Encoder-Decoder Model and Machine Translation Evaluation"
      ],
      "metadata": {
        "id": "rWttG4TaDYuY"
      },
      "id": "rWttG4TaDYuY"
    },
    {
      "cell_type": "markdown",
      "id": "b0e70652",
      "metadata": {
        "id": "b0e70652"
      },
      "source": [
        "# 1. Encoder-decoder model with cross-attention\n",
        "\n",
        "For deliverable 1, you'll implement an encoder-decoder model with cross-attention, as described in [SLP3 10.3](https://web.stanford.edu/~jurafsky/slp3/13.pdf).  Most of the model has been laid out for you.  Your task is to study the details as described in the book, and implement the `CrossAttentionBlock`, which allows tokens on the decoding side (e.g., a French translation you are generating) to attend to information on the encoding side (e.g., an original English sentence you're translating).  You'll implement the `CrossAttentionBlock` and the `forward` function within an Encoder-Decoder model that puts it all together to generate a sequence of tokens.  For this homework, only use the libraries that have been imported (don't import any new libraries).\n",
        "\n",
        "For all attention blocks (both self-attention and cross-attention), you'll only implement the attention layer within them; while attention blocks have other components (residual layer, layer normalization, feedforward layer), you can pretend they don't exist: the goal of is homework is for you to understand how *cross-attention* in particular works within an encoder-decoder model, so you can ignore those other components in doing so.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4727cf79",
      "metadata": {
        "id": "4727cf79"
      },
      "outputs": [],
      "source": [
        "from math import sqrt, log, exp\n",
        "import torch\n",
        "from torch import nn\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "torch.manual_seed(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4cbe5a1",
      "metadata": {
        "id": "c4cbe5a1"
      },
      "source": [
        "First, let's implement a standard self-attention block (again, forgetting about the residual layer, layer normalization, and feedforward layer)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c10933cf",
      "metadata": {
        "id": "c10933cf"
      },
      "outputs": [],
      "source": [
        "class SelfAttentionBlock(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim):  \n",
        "        super().__init__()\n",
        "\n",
        "        self.dim=hidden_dim\n",
        "        self.encoder_Wq=torch.nn.Linear(input_dim, hidden_dim) \n",
        "        self.encoder_Wk=torch.nn.Linear(input_dim, hidden_dim) \n",
        "        self.encoder_Wv=torch.nn.Linear(input_dim, input_dim)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x): \n",
        "\n",
        "        encoder_q=self.encoder_Wq(x)\n",
        "        encoder_k=self.encoder_Wk(x) \n",
        "        encoder_v=self.encoder_Wv(x)\n",
        "        mm=torch.mm(encoder_q, torch.transpose(encoder_k, 0, 1))\n",
        "\n",
        "        attention_weights=self.softmax(( mm )/sqrt(self.dim))\n",
        "        output=attention_weights @ encoder_v\n",
        "\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6923de27",
      "metadata": {
        "id": "6923de27"
      },
      "source": [
        "Using the SelfAttentionBlock as a model, implement the `CrossAttentionBlock` as illustrated in SLP3 figure 10.6 (and described in this section).  \n",
        "\n",
        "Keep in mind that the forward function of this method takes in information about `x` (the input tokens you are trying to translate) and `y` (the output tokens you are decoding).  In the `EncoderDecoder` class below, the input `x` to the forward function should be the `encoder_outputs`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70d1b37d",
      "metadata": {
        "id": "70d1b37d"
      },
      "outputs": [],
      "source": [
        "class CrossAttentionBlock(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim):  \n",
        "        super().__init__()\n",
        "\n",
        "        # BEGIN SOLUTION\n",
        "        # TO-DO \n",
        "        self.dim=None\n",
        "        self.encoder_Wq=None \n",
        "        self.encoder_Wk=None \n",
        "        self.encoder_Wv=None \n",
        "        self.softmax=None\n",
        "        \n",
        "        # END SOLUTION\n",
        "\n",
        "    def forward(self, x, y): \n",
        "\n",
        "        # BEGIN SOLUTION\n",
        "        # TO-DO \n",
        "        encoder_q=None \n",
        "        encoder_k=None \n",
        "        encoder_v=None\n",
        "        mm=None\n",
        "        \n",
        "        attention_weights=None\n",
        "        output=None\n",
        "        \n",
        "        # END SOLUTION\n",
        "\n",
        "        # Do not change the variable name!\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79d5718a",
      "metadata": {
        "id": "79d5718a"
      },
      "source": [
        "With those attention blocks, implement the encoder-decoding model as illustrated in SLP3 figure 10.6 (and described in this section).  Do not implement layer normalization, residual layers or feedforward layers; only the self-attention layers, cross-attention layer, and final linear layer.  We provide the components for you in the initializer; your task is to assemble them in the correct way in the forward function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21ccb38c",
      "metadata": {
        "id": "21ccb38c"
      },
      "outputs": [],
      "source": [
        "class EncoderDecoder(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_encoder_layers=2, num_decoder_layers=2, x_vocab_size=1000, y_vocab_size=1000):  \n",
        "        super().__init__()\n",
        "        \n",
        "        self.num_encoder_layers=num_encoder_layers\n",
        "        self.num_decoder_layers=num_decoder_layers\n",
        "\n",
        "        self.x_embeddings=torch.nn.Embedding(x_vocab_size,input_dim)\n",
        "        self.y_embeddings=torch.nn.Embedding(y_vocab_size,input_dim)\n",
        "        \n",
        "        self.encoder_blocks=[SelfAttentionBlock(input_dim, hidden_dim) for _ in range(self.num_encoder_layers)]\n",
        "\n",
        "        self.decoder_causal_blocks=[SelfAttentionBlock(input_dim, hidden_dim) for _ in range(self.num_decoder_layers)]\n",
        "        self.cross_attention_blocks=[CrossAttentionBlock(input_dim, hidden_dim) for _ in range(self.num_decoder_layers)]\n",
        "\n",
        "        self.linear_to_output_vocab=torch.nn.Linear(input_dim, y_vocab_size)\n",
        "\n",
        "    def forward(self, x): \n",
        "\n",
        "        # y_int is the sequence of token IDs that we've generated; let's assume vocab ID = 0 \n",
        "        # is the START symbol, so we'll initialize the list to that.\n",
        "        \n",
        "        y_int=[0]\n",
        "\n",
        "        # let's get encodings of x\n",
        "        \n",
        "        encoder_outputs=self.x_embeddings(x)\n",
        "        for i in range(self.num_encoder_layers):\n",
        "            encoder_outputs=self.encoder_blocks[i](encoder_outputs)\n",
        "\n",
        "        # now let's decode; for this exercise, we'll generate exactly 10 tokens (trained MT systems \n",
        "        # would stop when they predict the STOP symbol).\n",
        "        \n",
        "        for i in range(10):\n",
        "            \n",
        "            # let's get embeddings of the tokens we've generated so far\n",
        "            \n",
        "            y=self.y_embeddings(torch.LongTensor(y_int))\n",
        "\n",
        "            # you implement the rest. The output variable should be a matrix of size [ len(y_int), 1000 ]\n",
        "            \n",
        "            # BEGIN SOLUTION\n",
        "            # TO-DO (Do not change the variable name!)\n",
        "            output=None\n",
        "            # END SOLUTION  \n",
        "            \n",
        "            # from the output we'll select the vocab ID with the highest score and add that to token \n",
        "            # IDs we've generated\n",
        "                \n",
        "            choice=torch.argmax(output[-1])\n",
        "            y_int.append(choice)\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da6a6327",
      "metadata": {
        "id": "da6a6327"
      },
      "outputs": [],
      "source": [
        "model=EncoderDecoder(100, 37)\n",
        "input_x=torch.LongTensor([37, 2, 10])\n",
        "model.forward(input_x)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e1481dd",
      "metadata": {
        "id": "6e1481dd"
      },
      "source": [
        "# 2. Implement BLEU\n",
        "\n",
        "BLEU is a common method method for evaluation the performance of MT systems.  For this deliverable, you will implement BLEU and evaluate a sample translation with respect to several reference translations.  Refer to lecture, but here is the equation for BLEU:\n",
        "\n",
        "$$\n",
        "\\textrm{BLEU} = BP \\times \\exp {1\\over N} \\sum_{n=1}^N \\log p_n\n",
        "$$\n",
        "\n",
        "Where: \n",
        "$$\n",
        "p_n = {\\textrm{Number of ngram tokens in system and reference translations} \\over \\textrm{Number of ngram tokens in system translation}}\n",
        "$$\n",
        "\n",
        "And:\n",
        "\n",
        "The brevity penalty = $\\exp(1-r/c)$, where $c$ is the length of the hypothesis translation (in tokens), and $r$ is the length of the *closest* reference translation.\n",
        "\n",
        "Calculate BLEU for $N=4$.  Your code should return six values, in order:\n",
        "\n",
        "* bleu\n",
        "* $p_1$\n",
        "* $p_2$\n",
        "* $p_3$\n",
        "* $p_4$\n",
        "* the brevity penalty"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19e699f8",
      "metadata": {
        "id": "19e699f8"
      },
      "outputs": [],
      "source": [
        "hypothesis=\"Abandon all hope , ye who enter here\"\n",
        "references=[\"All hope abandon , ye who enter here\", \"All hope abandon , ye who enter in !\", \"Leave every hope, ye that enter\", \"Leave all hope , ye that enter\"]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# helper function to get n-grams from a given text\n",
        "def get_ngrams(text, order):\n",
        "    \"\"\"\n",
        "    Given a string `text` and an integer `order`, returns a Counter object containing\n",
        "    the frequency counts of all ngrams of size `order` in the string.\n",
        "    \"\"\"\n",
        "    ngrams = Counter()\n",
        "\n",
        "    # BEGIN SOLUTION\n",
        "    # TO-DO (Do not change the variable name!)\n",
        "    # END SOLUTION\n",
        "\n",
        "    return ngrams"
      ],
      "metadata": {
        "id": "FJIVWj99kub3"
      },
      "id": "FJIVWj99kub3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dict(get_ngrams(hypothesis, 2))) # sanity check: expected output should be\n",
        "# {'Abandon all': 1, 'all hope': 1, 'hope ,': 1, ', ye': 1, 'ye who': 1, 'who enter': 1, 'enter here': 1}"
      ],
      "metadata": {
        "id": "sSvc4iDUvaGQ"
      },
      "id": "sSvc4iDUvaGQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92a83614",
      "metadata": {
        "id": "92a83614"
      },
      "outputs": [],
      "source": [
        "def calculate_bleu(hypothesis, references):\n",
        "    \n",
        "    bleu=None\n",
        "    p1=None\n",
        "    p2=None\n",
        "    p3=None\n",
        "    p4=None\n",
        "    bp=None\n",
        "    \n",
        "    # BEGIN SOLUTION\n",
        "\n",
        "    # 1. find the closest reference to the hypothesis\n",
        "    closest_size=100000\n",
        "    closest_ref=None\n",
        "\n",
        "    for ref in references:\n",
        "        # TO-DO (Do not change the variable name!)\n",
        "        pass\n",
        "\n",
        "    # 2. calculate pn\n",
        "    pns=[]\n",
        "    for order in range(1,5):\n",
        "        # calculate intersection and union of n-grams\n",
        "        # hint: use the get_ngrams function you implemented\n",
        "        # calculate pn for each order\n",
        "        # TO-DO \n",
        "        pass\n",
        "\n",
        "    # 3. Calculate the brevity penalty\n",
        "    # TO-DO \n",
        "    bp=1\n",
        "    c=None\n",
        "    r=None\n",
        "\n",
        "    # 4. Calculate the BLEU score\n",
        "    # TO-DO \n",
        "    bleu=None\n",
        "    \n",
        "    # Don't forget to assign values to p1, p2, p3, p4!\n",
        "\n",
        "    # END SOLUTION\n",
        "    \n",
        "    # Do not change the variable name!\n",
        "    return bleu, p1, p2, p3, p4, bp\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "652852ec",
      "metadata": {
        "id": "652852ec"
      },
      "outputs": [],
      "source": [
        "bleu, p1, p2, p3, p4, bp=calculate_bleu(hypothesis, references)\n",
        "print(\"BLEU: %.3f\" % bleu) # sanity check: 0.5 < BLEU < 1"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}